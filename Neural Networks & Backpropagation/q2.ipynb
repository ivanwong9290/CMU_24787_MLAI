{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree, neighbors, linear_model, svm, neural_network, metrics"
   ]
  },
  {
   "source": [
    "# Problem 2a"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Generating Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('winequality-red.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[1:1001, 0:-1]\n",
    "train_Y = np.array([X[1:1001, -1]>=6]).astype(int).squeeze()\n",
    "test_X = X[1001::, 0:-1]\n",
    "test_Y = np.array([X[1001::, -1]>=6]).astype(int).squeeze()"
   ]
  },
  {
   "source": [
    "## Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Criterion: gini, Max Depth: 3, Training Accuracy: 73.8, Test Accuracy: 71.61936560934892.\n\nCriterion: gini, Max Depth: 5, Training Accuracy: 79.4, Test Accuracy: 72.95492487479132.\n\nCriterion: gini, Max Depth: 8, Training Accuracy: 89.1, Test Accuracy: 67.9465776293823.\n\nCriterion: entropy, Max Depth: 3, Training Accuracy: 73.4, Test Accuracy: 71.78631051752922.\n\nCriterion: entropy, Max Depth: 5, Training Accuracy: 77.7, Test Accuracy: 73.78964941569282.\n\nCriterion: entropy, Max Depth: 8, Training Accuracy: 86.9, Test Accuracy: 69.44908180300501.\n\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"gini\", \"entropy\"]\n",
    "max_depth = [3, 5, 8]\n",
    "for i in metrics:\n",
    "    for j in max_depth:\n",
    "        clf = tree.DecisionTreeClassifier(criterion=i, max_depth=int(j)).fit(train_X, train_Y)\n",
    "        train_preds = clf.predict(train_X)\n",
    "        test_preds = clf.predict(test_X)\n",
    "        train_acc = clf.score(train_X, train_Y)*100\n",
    "        test_acc = clf.score(test_X, test_Y)*100\n",
    "        print(\"Criterion: {}, Max Depth: {}, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(i, j, train_acc, test_acc))"
   ]
  },
  {
   "source": [
    "## K-Nearest Neighbors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "N-neighbors: 8, Training Accuracy: 75.5, Test Accuracy: 58.931552587646074.\n",
      "\n",
      "N-neighbors: 12, Training Accuracy: 74.1, Test Accuracy: 59.59933222036727.\n",
      "\n",
      "N-neighbors: 25, Training Accuracy: 71.8, Test Accuracy: 62.604340567612695.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = [8, 12, 25]\n",
    "for i in n:\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=i).fit(train_X, train_Y)\n",
    "    train_preds = clf.predict(train_X)\n",
    "    test_preds = clf.predict(test_X)\n",
    "    train_acc = clf.score(train_X, train_Y)*100\n",
    "    test_acc = clf.score(test_X, test_Y)*100\n",
    "    print(\"N-neighbors: {}, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(i, train_acc, test_acc))"
   ]
  },
  {
   "source": [
    "## Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Penalty: l1, Training Accuracy: 72.8, Test Accuracy: 75.45909849749583.\n",
      "\n",
      "Penalty: l2, Training Accuracy: 73.7, Test Accuracy: 76.62771285475793.\n",
      "\n",
      "Penalty: elasticnet, Training Accuracy: 72.8, Test Accuracy: 75.45909849749583.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X_wbias = np.hstack((train_X, np.ones_like(train_Y)[:, np.newaxis]))\n",
    "test_X_wbias = np.hstack((test_X, np.ones_like(test_Y)[:, np.newaxis]))\n",
    "reg_method = [\"l1\", \"l2\", \"elasticnet\"]\n",
    "for i in reg_method:\n",
    "    if i == \"l1\":\n",
    "        clf = linear_model.LogisticRegression(penalty=i, solver=\"saga\", max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "    elif i == \"elasticnet\":\n",
    "        clf = linear_model.LogisticRegression(penalty=i, solver=\"saga\", l1_ratio=1, max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "    else:\n",
    "        clf = linear_model.LogisticRegression(penalty=i, max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "    train_preds = clf.predict(train_X_wbias)\n",
    "    test_preds = clf.predict(test_X_wbias)\n",
    "    train_acc = clf.score(train_X_wbias, train_Y)*100\n",
    "    test_acc = clf.score(test_X_wbias, test_Y)*100\n",
    "    print(\"Penalty: {}, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(i, train_acc, test_acc))"
   ]
  },
  {
   "source": [
    "## Support Vector Machine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kernel: linear, C: 0.01, Training Accuracy: 72.0, Test Accuracy: 72.45409015025042.\n",
      "\n",
      "Kernel: linear, C: 10, Training Accuracy: 73.3, Test Accuracy: 76.62771285475793.\n",
      "\n",
      "Kernel: linear, C: 1000, Training Accuracy: 73.2, Test Accuracy: 75.79298831385643.\n",
      "\n",
      "Kernel: rbf, C: 0.01, Training Accuracy: 62.3, Test Accuracy: 60.60100166944908.\n",
      "\n",
      "Kernel: rbf, C: 10, Training Accuracy: 71.5, Test Accuracy: 71.28547579298832.\n",
      "\n",
      "Kernel: rbf, C: 1000, Training Accuracy: 77.5, Test Accuracy: 74.95826377295492.\n",
      "\n",
      "Kernel: poly, Training Accuracy: 77.5, Test Accuracy: 74.95826377295492.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernels = [\"linear\", \"rbf\"]\n",
    "Cs = [0.01, 10, 1000]\n",
    "for i in kernels:\n",
    "    for j in Cs:\n",
    "        clf = svm.SVC(C=j, kernel=i).fit(train_X, train_Y)\n",
    "        train_preds = clf.predict(train_X)\n",
    "        test_preds = clf.predict(test_X)\n",
    "        train_acc = clf.score(train_X, train_Y)*100\n",
    "        test_acc = clf.score(test_X, test_Y)*100\n",
    "        print(\"Kernel: {}, C: {}, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(i, j, train_acc, test_acc))\n",
    "clf = svm.SVC(kernel=\"poly\").fit(train_X, train_Y)\n",
    "print(\"Kernel: poly, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(train_acc, test_acc))"
   ]
  },
  {
   "source": [
    "## Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Set 1: Activation Function: logistic, Training Accuracy: 75.0, Test Accuracy: 75.79298831385643.\n",
      "\n",
      "Set 1: Activation Function: tanh, Training Accuracy: 75.8, Test Accuracy: 73.12186978297161.\n",
      "\n",
      "Set 1: Activation Function: relu, Training Accuracy: 74.4, Test Accuracy: 73.62270450751252.\n",
      "\n",
      "Set 2: Activation Function: logistic, Training Accuracy: 74.7, Test Accuracy: 76.12687813021702.\n",
      "\n",
      "Set 2: Activation Function: tanh, Training Accuracy: 76.6, Test Accuracy: 71.28547579298832.\n",
      "\n",
      "Set 2: Activation Function: relu, Training Accuracy: 76.4, Test Accuracy: 72.78797996661102.\n",
      "\n",
      "Set 3: Activation Function: logistic, Training Accuracy: 74.7, Test Accuracy: 75.79298831385643.\n",
      "\n",
      "Set 3: Activation Function: tanh, Training Accuracy: 76.4, Test Accuracy: 72.45409015025042.\n",
      "\n",
      "Set 3: Activation Function: relu, Training Accuracy: 75.4, Test Accuracy: 77.46243739565944.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "act_fun = [\"logistic\", \"tanh\", \"relu\"]\n",
    "for i in act_fun:\n",
    "    clf = neural_network.MLPClassifier(hidden_layer_sizes=(10, 20, 10), activation=i, max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "    train_preds = clf.predict(train_X_wbias)\n",
    "    test_preds = clf.predict(test_X_wbias)\n",
    "    train_acc = clf.score(train_X_wbias, train_Y)*100\n",
    "    test_acc = clf.score(test_X_wbias, test_Y)*100\n",
    "    print(\"Set 1: Activation Function: {}, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(i, train_acc, test_acc))\n",
    "for i in act_fun:\n",
    "    clf = neural_network.MLPClassifier(hidden_layer_sizes=(20, 50, 10), activation=i, max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "    train_preds = clf.predict(train_X_wbias)\n",
    "    test_preds = clf.predict(test_X_wbias)\n",
    "    train_acc = clf.score(train_X_wbias, train_Y)*100\n",
    "    test_acc = clf.score(test_X_wbias, test_Y)*100\n",
    "    print(\"Set 2: Activation Function: {}, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(i, train_acc, test_acc))\n",
    "for i in act_fun:\n",
    "    clf = neural_network.MLPClassifier(hidden_layer_sizes=(30, 70, 10), activation=i, max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "    train_preds = clf.predict(train_X_wbias)\n",
    "    test_preds = clf.predict(test_X_wbias)\n",
    "    train_acc = clf.score(train_X_wbias, train_Y)*100\n",
    "    test_acc = clf.score(test_X_wbias, test_Y)*100\n",
    "    print(\"Set 3: Activation Function: {}, Training Accuracy: {}, Test Accuracy: {}.\\n\".format(i, train_acc, test_acc))"
   ]
  },
  {
   "source": [
    "# Problem 2b"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A confusion matrix is a matrix that takes a prediction $i$ and try to categorize it with to a ground truth $j$, if $i$ and $j$ does not match, it is a false positive or negative, otherwise, it is a true positive and negative."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A precision takes components from confusion matrix (true positives and false positives) and put it in a ratio such that it represents a performance/efficacy value for the prediction algorithm.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A recall matrix components acts as a precision, but on the true positive and false negative from the confusion matrix. It is a measurement of the ability of the prediction algorithm to output positive values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A F1 matrix is the weighted average between the recall and precision."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Best parameters from each category"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Decision Tree: Criterion = entropy, Max Depth = 5, Training Accuracy: 77.7, Test Accuracy: 73.45575959933221."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix =\n [[145  94]\n [ 65 295]]\nPrecision =  0.7583547557840618\nRecall =  0.8194444444444444\nF1 =  0.787716955941255\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=5).fit(train_X, train_Y)\n",
    "test_preds = clf.predict(test_X)\n",
    "confusion = metrics.confusion_matrix(test_Y, test_preds)\n",
    "precision = metrics.precision_score(test_Y, test_preds)\n",
    "recall = metrics.recall_score(test_Y, test_preds)\n",
    "f1 = metrics.f1_score(test_Y, test_preds)\n",
    "print(\"Confusion Matrix =\\n\", confusion)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)"
   ]
  },
  {
   "source": [
    "### KNN: N-neighbors = 25, Training Accuracy: 71.8, Test Accuracy: 62.604340567612695."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix =\n [[133 106]\n [118 242]]\nPrecision =  0.6954022988505747\nRecall =  0.6722222222222223\nF1 =  0.6836158192090396\n"
     ]
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors=25).fit(train_X, train_Y)\n",
    "test_preds = clf.predict(test_X)\n",
    "confusion = metrics.confusion_matrix(test_Y, test_preds)\n",
    "precision = metrics.precision_score(test_Y, test_preds)\n",
    "recall = metrics.recall_score(test_Y, test_preds)\n",
    "f1 = metrics.f1_score(test_Y, test_preds)\n",
    "print(\"Confusion Matrix =\\n\", confusion)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)"
   ]
  },
  {
   "source": [
    "### Logistic Regression: Penalty = l2, Training Accuracy: 73.7, Test Accuracy: 76.62771285475793."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix =\n [[156  83]\n [ 57 303]]\nPrecision =  0.7849740932642487\nRecall =  0.8416666666666667\nF1 =  0.8123324396782843\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "test_preds = clf.predict(test_X_wbias)\n",
    "confusion = metrics.confusion_matrix(test_Y, test_preds)\n",
    "precision = metrics.precision_score(test_Y, test_preds)\n",
    "recall = metrics.recall_score(test_Y, test_preds)\n",
    "f1 = metrics.f1_score(test_Y, test_preds)\n",
    "print(\"Confusion Matrix =\\n\", confusion)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)"
   ]
  },
  {
   "source": [
    "### Support Vector Machine: Kernel = linear, C = 10, Training Accuracy: 73.3, Test Accuracy: 76.62771285475793."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix =\n [[170  69]\n [ 71 289]]\nPrecision =  0.8072625698324022\nRecall =  0.8027777777777778\nF1 =  0.8050139275766017\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=10, kernel=\"linear\").fit(train_X, train_Y)\n",
    "test_preds = clf.predict(test_X)\n",
    "confusion = metrics.confusion_matrix(test_Y, test_preds)\n",
    "precision = metrics.precision_score(test_Y, test_preds)\n",
    "recall = metrics.recall_score(test_Y, test_preds)\n",
    "f1 = metrics.f1_score(test_Y, test_preds)\n",
    "print(\"Confusion Matrix =\\n\", confusion)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)"
   ]
  },
  {
   "source": [
    "### Neural Network: Hidden Layer Sizes = 20, 50, 10, Activation Function = Logistic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix =\n [[159  80]\n [ 77 283]]\nPrecision =  0.7796143250688705\nRecall =  0.7861111111111111\nF1 =  0.7828492392807745\n"
     ]
    }
   ],
   "source": [
    "clf = neural_network.MLPClassifier(hidden_layer_sizes=(20, 50, 10), activation=i, max_iter=100000).fit(train_X_wbias, train_Y)\n",
    "train_preds = clf.predict(train_X_wbias)\n",
    "test_preds = clf.predict(test_X_wbias)\n",
    "confusion = metrics.confusion_matrix(test_Y, test_preds)\n",
    "precision = metrics.precision_score(test_Y, test_preds)\n",
    "recall = metrics.recall_score(test_Y, test_preds)\n",
    "f1 = metrics.f1_score(test_Y, test_preds)\n",
    "print(\"Confusion Matrix =\\n\", confusion)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", f1)"
   ]
  },
  {
   "source": [
    "## Case study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Case A: I would sell the Logistic Regression model in this case because I want to maximize the number of positive value (good wine), therefore, I should look for the model with the highest Recall Score."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Case B: I would sell the Logisitic Regression model again in this case. The customer does not want to miss too many good wines, this implies that she wants a good recall score on the good wines. However, she also does not desire when classified good wines to actually taste bad (false positives), which implies a precision scoring. Together, I conclude that the best F1 score will satisfy her the most, which the Logistic Regression model is able to offer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}